# Clear Channel QA Engineer Recruitment Test

## Welcome
Thank you for taking the time to do our technical test. 

The test is comprised of four parts:
1. Review an example BDD cucumber scenario and provide suggestions on how to improve it
2. Create 2 new cucumber scenarios and automate them
3. Create a readme which details how others can run the two scenarios
4. Build a simple JSON object 

## Submitting your test

Your submitted test should be hosted on Github (or another VCS of your choice).

Once you have completed the test, please send the link to the recruiter, who will forward this on to the QA Manager at Clear Channel International.

## Test instructions

### TASK 1: Peer review

Being part of the QA team, your role will include peer reviewing BDD scenarios (written using the Gherkin syntax) as well as step definitions written by other members of the QA team. 

In this repo there is a file named ``search.feature`` Imagine this has been created by a Junior QA Engineer on your team. 

Please peer review the scenario to see if you can spot any issues or potential improvements. 

Please create a file named 'peer_review.md' and include the following:
* List each issue you see with the scenario and state how you would improve it
* Implement the changes you have suggested

### TASK 2: Create Cucumber test scenarios

1. Go to http://the-internet.herokuapp.com/login
2. Create a file named ``login.feature`` and add test scenarios to test the login functionality on the page
3. Build the automation framework to automate two of the scenarios you have written. 
You can use any language, framework and libraries of your choice. 

### TASK 3: Create a readme

Please create a markdown readme file named ``readme.md`` which includes clear instructions on how to execute your test scenarios. 

### TASK 4: Create a JSON file
Please create a simple JSON file which includes an array. This can be about any topic of your choosing e.g. describe your perfect car

## What we are assessing:

**After you have submitted your technical test, we will be assessing the following:**

* **Identifying opportunities for improvement** - what changes have you made to the sample scenario? Have these changes improved it? 
* **Code quality** - do the tests compile and run? Does each scenario test what is is meant to? 
* **Readibility** - are the scenarios, automation code and JSON written in a way that is easy for others to understand? Does the Readme provide adequate intructions?
